{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e374df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import os\n",
    "from gym import Env\n",
    "from gym import spaces\n",
    "from gym.spaces import Box, Discrete\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.envs.classic_control import rendering\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666b787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two points in screen.\n",
    "def dist_calculator(point_a, point_b):\n",
    "    return np.linalg.norm(point_a - point_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cc4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GazeEnv(Env):\n",
    "    def __init__(self, width, distance, ocular_noise, spatial_noise):\n",
    "        self.width = math.radians(width) # Diameter of target in radians\n",
    "        self.distance = math.radians(distance) # Distance from start to target in radians\n",
    "        self.ocular_noise = ocular_noise # Oculomotor noise\n",
    "        self.spatial_noise = spatial_noise # Visual spatial noise\n",
    "        self.theta = None # Angle of target from start position\n",
    "        self.current_pos = np.array([0, 0]) # Current gaze position or fixation\n",
    "        self.max_fixations = 10000 # Maximum number of saccades allowed\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float64) # Gaze fixation\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float64) # Current gaze position\n",
    "        self.state_space = spaces.Box(low=-1, high=1, shape=(2, ), dtype=np.float64) # Target location\n",
    "        self.belief_space = spaces.Box(low=-1, high=1, shape=(2, ), dtype=np.float64)\n",
    "        self.reward_range = (-1, 0) # Reward range\n",
    "        #self.viewer = None # Viewer for rendering\n",
    "        \n",
    "        #To visualise the env.\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((1080, 720))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self):\n",
    "        self.theta = np.random.uniform(low=0, high=2*np.pi)\n",
    "        self.target_pos = np.array([self.distance * np.cos(self.theta), self.distance * np.sin(self.theta)])\n",
    "        self.current_pos = np.array([0, 0])\n",
    "        self.num_fixations = 1 # 1 because gaze is fixated at [0,0] initially\n",
    "        self.observation, self.observation_uncertainity = self.get_observation()\n",
    "        self.belief, self.belief_uncertainity = self.observation, self.observation_uncertainity #As per paper, belief is based on agent's observation\n",
    "        \n",
    "        return self.belief\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        saccade_movement = dist_calculator(self.current_pos, action)\n",
    "        noise_std = np.random.normal(0, self.ocular_noise*saccade_movement, action.shape) #Generate random noise from gaussian distribution- \".normal(loc, scale, size)\"\" \n",
    "        self.current_pos = np.clip(action + noise_std, -1, 1)\n",
    "        \n",
    "        self.num_fixations += 1\n",
    "        \n",
    "        #Check the distance between gaze & target.\n",
    "        dist_btw_gaze_and_target = dist_calculator(self.current_pos, self.target_pos)\n",
    "        \n",
    "        if dist_btw_gaze_and_target < self.width/2: #As per paper if distance is less half of target width, the gaze is in target region.\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            self.observation, self.observation_uncertainity = self.get_observation()\n",
    "            self.belief, self.belief_uncertainity = self.get_belief()\n",
    "            \n",
    "        if self.num_fixations > self.max_fixations:\n",
    "            done = True\n",
    "            \n",
    "        \n",
    "        #dict to store other essential values.\n",
    "        addon_dict = { 'target_pos' : self.target_pos,\n",
    "                       'current_pos': self.current_pos,\n",
    "                       'belief': self.belief,\n",
    "                       'action': action,\n",
    "                       'num_fixation': self.num_fixations}\n",
    "\n",
    "        return self.belief, reward, done, addon_dict\n",
    "    \n",
    "    def get_observation(self):\n",
    "        gaze_displacement = dist_calculator(self.target_pos,self.current_pos) #gaze eccentricity\n",
    "        observation_uncertainty = gaze_displacement\n",
    "        spatial_noise=np.random.normal(0, self.spatial_noise*gaze_displacement, self.target_pos.shape) # visual spatial noise is calculated by gaze & target eccentricity.\n",
    "        observation=np.clip(self.target_pos + spatial_noise, -1, 1)\n",
    "        \n",
    "        return observation, observation_uncertainty\n",
    "    \n",
    "    def get_belief(self):\n",
    "        new_observation, new_observation_uncertainity = self.observation, self.observation_uncertainity\n",
    "        prev_belief, prev_belief_uncertainity = self.belief, self.belief_uncertainity\n",
    "        scale_obs = pow(prev_belief_uncertainity, 2) / (pow(new_observation_uncertainity, 2) + pow(prev_belief_uncertainity, 2))\n",
    "        scale_belief = pow(new_observation_uncertainity, 2) / (pow(new_observation_uncertainity, 2) + pow(prev_belief_uncertainity, 2))\n",
    "        new_belief = scale_obs * prev_belief + scale_belief * new_observation\n",
    "        new_belief_uncertainity = (pow(prev_belief_uncertainity, 2) * pow(new_observation_uncertainity, 2)) / (pow(new_observation_uncertainity, 2) + pow(prev_belief_uncertainity, 2))\n",
    "        \n",
    "        return new_belief, new_belief_uncertainity\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        # Fill the screen with white\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        screen_width = 1080\n",
    "        screen_height = 720\n",
    "        \n",
    "        world_width = 2 #To be consistent with (x,y) coordinates of agent's position\n",
    "        world_height = 2\n",
    "        scale = screen_width/world_width\n",
    "        radius = int(self.width*scale/2)\n",
    "        target_pos_pix = np.array([int(self.target_pos[0]*scale+screen_width/2), int(self.target_pos[1]*scale+screen_height/2)])\n",
    "        current_pos_pix = np.array([int(self.current_pos[0]*scale+screen_width/2), int(self.current_pos[1]*scale+screen_height/2)])\n",
    "        \n",
    "        #To nullify pygame not responding problem.\n",
    "        pygame.event.get()\n",
    "        \n",
    "        #Draw target position\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), target_pos_pix, radius)\n",
    "        \n",
    "        #Draw gaze position\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), current_pos_pix, 10)\n",
    "        \n",
    "        # Update the display\n",
    "        pygame.display.update()\n",
    "        \n",
    "        return radius, scale, current_pos_pix, target_pos_pix, self.target_pos[0], self.target_pos[1], self.current_pos[0], self.current_pos[1], self.target_pos, self.current_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ea9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(w, d, ocular_noise, spatial_noise, run):\n",
    "    env = GazeEnv(w, d, ocular_noise, spatial_noise)\n",
    "    log_path = r\"D:\\Research-Project\\infotech2023_jayakumar\\log-directory\"\n",
    "    model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "    model.learn(total_timesteps=400000)\n",
    "    print('run', run)\n",
    "    model_path = os.path.join('saved-models', 'PPO_Gaze_Model_4L_trained_all')\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f033749",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GazeEnv(1.5, 10, 0.08, 0.09) #width and distance in degrees of visual angle as mentioned in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48d434e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = np.array([5, 4, 3, 2, 1.5, 1])\n",
    "distance = np.array([5, 10])\n",
    "ocular_noise=0.07\n",
    "spatial_noise=0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04956df",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "for w in width:\n",
    "            for d in distance:\n",
    "                run+=1        \n",
    "                train(w, d, ocular_noise, spatial_noise, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af69666",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8aaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 20\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        #env.save_image(episode)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc38bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3906281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c797f0d",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c29418",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = r\"D:\\Research-Project\\infotech2023_jayakumar\\log-directory\"\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71731465",
   "metadata": {},
   "source": [
    "To Save the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe7fb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('saved-models', 'PPO_Gaze_Model_50L_trained_1w_10d_10ksteps')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('D:\\Research-Project\\infotech2023_jayakumar\\saved-models\\PPO_Gaze_Model_50L_trained.zip', env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2272ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, no_use = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #env.save_image(episode)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
